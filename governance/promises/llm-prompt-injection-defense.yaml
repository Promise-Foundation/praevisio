id: llm-prompt-injection-defense
version: 0.1.0
domain: /llm/safety
statement: The system must block prompt injection attempts at the boundary.
critical: true
success_criteria:
  credence_threshold: 0.90
  evidence_types:
    - procedural
parameters: {}
stake:
  credits: 0
